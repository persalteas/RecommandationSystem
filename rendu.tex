\documentclass[12pt,a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage[francais]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subfig}


\begin{document}
\title{Algèbre Linéaire et Analyse Matricielle\\ Projet: \\ Systèmes de recommandation do-it-yourself}
\date{\today}
\author{Louis Becquey - 3BIM}
\maketitle
\newpage

\section{Question A - Chargez les données}
Les données du site movielens.org sont chargées dans des dictionnaires python.\\

Voir: \textit{http://grouplens.org/datasets/movielens/100k/}
\section{Question B - Matrice Utilisateur-Item}
On construit une matrice R de 943 lignes (les utilisateurs) et 1682 colonnes (les films).
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{R-Not-Filled.png}
	\caption{Matrice $R$ de données: Bleu=Pas de vote, couleurs du vert au rouge=note de 1 à 5 étoiles.}
\end{figure}
\\
On remarque qu'il existe des films qui n'ont été notés par personne, cependant, il n'existe pas d'utilisateur qui n'aie voté pour aucun film.
\newpage

\section{Question C - Approximation bas-rang par SVD}
On remplit les données manquantes de la matrice $R$ de deux façons différentes, en utilisant la moyenne des notes de ce film, ou en utilisant la moyenne des notes de cet utilisateur.\\

\begin{figure}[h!]
	\includegraphics[scale=0.28]{R-Filled-By-Movie.png}
	\includegraphics[scale=0.28]{R-Filled-By-User.png}
	\caption{A droite: Matrice $Rc$ remplie avec la moyenne du film lors de données manquantes. A gauche: Matrice $Rr$ remplie avec la moyenne de l'utilisateur lors de données manquantes.}
\end{figure}

On utilisera \texttt{ numpy.linalg.svd() } pour calculer les décompositions en valeurs singulières. $$R=USV'$$On réalise ensuite l'approximation de $Rc$ ou $Rr$ en ne retenant que $k$ valeurs singulières sur $m$.\\ Éliminer les $(m-k)$ dernières valeurs singulières revient à éliminer dans le calcul les valeurs des $(m-k)$ dernières colonnes de $U$ et des $(m-k)$ dernières lignes de $V'$.\\
Donc à partir de $U$ de taille $m\times m$, $S$ de taille $m \times m$ et $V'$ de taille $m \times n$, on ne garde que $U_k$ une matrice de taille $m \times k$, $S_k$ de taille $k \times k$ et $V'_k$ de taille $k \times n$.
$$R \approx U_kS_kV'_k$$

\begin{figure}
\centering
\includegraphics[scale=0.2]{R-Approximation-By-Movie.png}
\caption{Exemple d'approximation de R (remplie avec les moyennes des films) en ne conservant que 30 valeurs singulières}
\end{figure}

\newpage
\section{Question D - Qualité de la prédiction par SVD}
On peut estimer l'erreur par la moyenne (en note) des différences entre la note réelle dans l'échantillon test et celle prévue par l'approximation de rang k (en valeur absolue), dite MAE (Mean Absolute Error).\\

\begin{figure}[h!]
	\includegraphics[scale=0.38]{MAE-By-Movie.png}
	\includegraphics[scale=0.38]{MAE-By-User.png}
	\caption{A droite: MAE selon le rang de l'approximation, pour Rc (moyenne du film) à gauche, et pour Rr (moyenne de l'utilisateur) à droite.}
\end{figure}

On remarque donc que la méthode naïve utilisant la moyenne du film est légèrement plus fidèle.\\
En prenant k=12, on arrive à se limiter à une erreur d'environ 0,792 par note.\\
Il est important de se rappeler que cette note est attribuée en étoiles de 1 à 5, sans décimales. On pourrait donc arrondir l'erreur et considérer que cette méthode est précise à plus ou moins 1 étoile.\\

Si ensuite on souhaite mesurer l'erreur due à l'approximation par SVD, on peut là aussi faire la moyenne des différences (en valeur absolue) entre la matrice remplie par méthode naïve et l'approximation de cette même matrice, avec k=12.\\
On obtient, pour Rr comme pour Rc, une erreur d'environ 0,06 étoile, ce qui est faible et tout à fait satisfaisant.\\

En effet, l'approximation de la matrice permet d'\textbf{économiser énormément d'espace de stockage} (dans notre exemple: $943\times1682=1586126$ contre $943\times12+1682\times12+12\times12=31644$ soit un facteur 50). En revanche, le \textbf{temps de calcul est plus long} puisque l'on doit calculer deux produits pour obtenir le résultat. Et ceci pour une \textbf{différence de qualité de précision faible} étant donné la MAE de la méthode en général face aux réelles valeurs de l'échantillon test (0,06 contre 0,79).

\newpage
\section{Question E - Théorie}
Montrons que si une matrice A est réelle, alors elle admet une décomposition en valeurs singulières $A=U \Sigma V^{*}$, avec U et V des matrices à coefficients réels.

On a U matrice $m*m$ unitaire, V matrice $n*n$ unitaire et $\Sigma$ matrice diagonale à coefficients réels positifs $m*n$.\\

On sait que la matrice $A^*A$ est une matrice réelle symétrique $n*n$ à valeurs positives. Ainsi, elle est diagonalisable et admet des valeurs propres réelles positives $(\lambda_1,...,\lambda_n)$ et une base orthonormée de vecteurs propres associés ($V_1,...,V_n$) tels que $$A^*AV_i=\lambda_i V_i$$
et $$ V_i^* A^*AV_j=\lambda_j V_i^* V_j = \lambda_j \delta_{ij}$$

On pose $\sigma_j=\sqrt{\lambda_j}$ et pour les $\sigma_j>0$ (par exemple pour j=0,...,r), $U_j=\frac{AV_j}{\sigma_j}$.\\
Les ($U_1,...,U_r$) forment une famille orthonormée de I\!R$^r$ que l'on prolonge en base orthonormée de I\!R$^m$. Alors, on obtient:
$$(U^*AV)_{ij} = U_i^*AV_j = \frac{V_i^*A^*AV_j}{\sigma_i} = \frac{\lambda_j \delta_{ij}}{\sigma_i} = \sigma_j \delta_{ij} $$
pour $j\leq r $ et 0 sinon.\\

On a donc $U^* A V = \Sigma$, donc $A=U \Sigma V^{*}$. On a prouvé que U et V sont des matrices à coefficients réels.\\

De plus, on a les égalités suivantes:
$$ AA^*= U \Sigma V^{*} V \Sigma^* U^*  =U\Sigma\Sigma^*U^* $$
$$ A^*A= A= V \Sigma^* U^* U \Sigma V^{*} = V \Sigma^* \Sigma V^*$$
Ainsi, les matrices $U$ et $V$ contiennent les vecteurs propres associés à $AA^*$ et $A^*A$ et $\Sigma$ contient les racines carrées des valeurs propres associées à $AA^*$ et $A^*A$.

\newpage
\section{Question F - Problème des moindres carrés pour méthode alternée}
On a pour objectif d'obtenir $X.Y\approx R$. Le rôle de la ligne $X_u$ dans ce produit est de permettre de calculer les valeurs de la ligne $R_u$ dans la matrice finale. Ainsi, 
$$X_u.Y\approx R_u$$
$$(X_u.Y)^T\approx (R_u)^T$$
$$Y^T.X_u^T\approx R_u^T$$
où $Y^T$ est une matrice $n\times k$, et $X_u^T$ et $R_u^T$ deux vecteurs colonnes, $R_u^T$ connu. \\
Il s'agit donc d'un problème des moindres carrés linéaire.\\

Et plus directement, le rôle de la colonne $Y_i$ est de permettre le calcul des valeurs de la colonne $R_i$, donc:
$$X.Y_i \approx R_i$$
Il s'agit là aussi d'un problème des moindres carrés linéaire.\\

Choisissons donc une matrice Y arbitraire, par exemple pleine de 1. On va donc résoudre le problème linéaire $Y^T.X_u^T= R_u^T$ pour chaque ligne $X_u$, ce qui nous donnera une matrice X. Connaissant cette matrice X, on résout le problème $X.Y_i = R_i$ pour préciser notre matrice Y. On répète ce schéma un certain nombre de fois en espérant que X et Y convergent vers une solution qui minimise l'erreur moyenne absolue. On obtient les résultats suivants:

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{least-squares-MAE.png}
\caption{Des MAE de l'ordre de $10^{12}$ pour 250 itérations...}
\end{figure}
On remarque donc que cette méthode converge, mais beaucoup trop lentement à notre goût. \textit{(Diminution d'une puissance de 10 en moyenne en 200 itérations = 10 minutes de calcul)}

\newpage
Cherchons donc plutôt des matrices X et Y de taille plus petite: $943\times k$ et $k\times 1682$.\\
Puis restreignons les calculs aux coefficients non nuls de R, et accordons nous une erreur $\lambda$ au sens des moindres carrés. On cherche donc les lignes u de X et les colonnes i de Y telles que la quantité
$$\sum_{u,i} (R_{u,i}-X_uY_i)^2+\lambda \left(\sum_u ||X_u||^2 + \sum_i ||X_i||^2 \right) \text{soit minimisée.}$$

On crée donc une matrice W de même taille que R, dont les valeurs sont 1 si $R_{u,i} \neq 0$, zéro sinon. Cela revient à minimiser les fonctions suivantes sur $X_u$ et $Y_i$ :
$$\begin{array}{l}
\text{Pour Y fixé, } F(X_u) = (R_u -X_uY) diag(W_u) (R_u -X_uY)^* + \lambda X_u X_u^*\\
\text{Pour X fixé, } G(Y_i) = (R_i -XY_i)^* diag(W_i) (R_i -XY_i) + \lambda Y_i^*Y_i\\
\end{array}$$
Le minimum local de ces deux fonctions correspondra au minimum local de la quantité ci-dessus. On cherche donc ce point, pour lequel $\frac{\partial F}{\partial X_u}=\frac{\partial G}{\partial Y_i}=0$.

$$F(X_u) = (R_u -X_uY) diag(W_u) (R_u -X_uY)^* + \lambda X_u X_u^*$$
$$F(X_u) = (R_u^* -Y^*X_u^*)^* diag(W_u) (R_u^* -Y^*X_u^*) + \lambda X_u X_u^*$$
$$0=\frac{dF(X_u)}{dX_u}=\frac{dx}{dX_u}\frac{dF}{dx}+\lambda \frac{d}{dX_u}(X_uX_u^*) \text{ avec } x=R_u^* -Y^*X_u^*$$
$$0=-Y.2.diag(W_u)(R_u^*-Y^*X_u^*)+2.\lambda.X_u^*$$
$$0=Y.diag(W_u).(Y^*X_u^*-R_u^*)+\lambda X_u^*$$
$$0=Y.diag(W_u).Y^*X_u^*-Y.diag(W_u).R_u^*+\lambda X_u^*$$
$$\text{Donc }(Y.diag(W_u).Y^*+\lambda I)X_u^* = Y.diag(W_u).R_u^*$$
\begin{center}
\fbox{$X_u=\left[ (Y.diag(W_u).Y^*+\lambda I)^{-1}.(Y.diag(W_u).R_u^*) \right]^*$}
\end{center}
Cette équation normale nous donne la capacité d'estimer X quand on connaît R, W, $\lambda$, et Y. On peut faire de même pour trouver une expression de $Y_i$:
$$G(Y_i) = (R_i -XY_i)^* diag(W_i) (R_i -XY_i) + \lambda Y_i^*Y_i$$
$$0=\frac{dG(Y_i)}{dY_i}=\frac{dx}{dY_i}\frac{dG}{dx}+\lambda \frac{d}{dY_i}(Y_i^*Y_i)\text{ avec }x=R_i-XY_i$$
$$0=-X^*.2.diag(W_i)(R_i -XY_i) + \lambda 2 Y_i$$
$$0=X^*diag(W_i)(XY_i-R_i)+\lambda Y_i$$
$$0=X^*.diag(W_i)XY_i-X^*diag(W_i)R_i+\lambda Y_i$$
$$\text{Donc }(X^*diag(W_i)X+\lambda I)Y_i=X^*diag(W_i)R_i$$
\begin{center}
\fbox{$Y_i = (X^*diag(W_i)X+\lambda I)^{-1}.(X^*diag(W_i)R_i)$}
\end{center}
On a transformé le problème des moindres carrés non linéaire en deux problèmes linéaires (\textit{de type $A.x=b$ avec A et b connus}), qu'on va résoudre par une approche itérative: prendre deux matrices X et Y arbitraires (pleines de 1), estimer X, puis estimer Y avec ce X, puis recommencer un certain nombre d'itérations en espérant converger vers des solutions minimisant la MAE:
\newpage
\begin{table}[h]
\centering
\begin{tabular}{cc}
\subfloat[Pour k=3 ; $\lambda=0.02$]{\includegraphics[scale=0.4]{MAE-least-squares-k3-l2.png}}
&
\subfloat[Pour k=3 ; $\lambda=0.80$]{\includegraphics[scale=0.4]{MAE-least-squares-k3-l80.png}}
\end{tabular}
\end{table}

\begin{table}[h]
\begin{tabular}{cccc}
\subfloat[X*Y à la $1^{ere}$ itération]{\includegraphics[scale=0.2]{it1-k3-l2.png}}
&
\subfloat[X*Y à la $2^e$ itération \textit{(meilleure MAE)}]{\includegraphics[scale=0.2]{it2-k3-l2.png}}
&
\subfloat[X*Y à la $3^e$ itération]{\includegraphics[scale=0.2]{it3-k3-l2.png}}
&
\subfloat[X*Y à la $10^e$ itération]{\includegraphics[scale=0.2]{it10-k3-l2.png}}
\end{tabular}
\caption{Pour k=3, $\lambda$=0.02}
\end{table}

\begin{table}[h]
\begin{tabular}{cccc}
\subfloat[X*Y à la $1^{ere}$ itération]{\includegraphics[scale=0.2]{it1-k3-l80.png}}
&
\subfloat[X*Y à la $2^e$ itération]{\includegraphics[scale=0.2]{it2-k3-l80.png}}
&
\subfloat[X*Y à la $10^e$ itération]{\includegraphics[scale=0.2]{it10-k3-l80.png}}
&
\subfloat[X*Y à la $50^e$ itération]{\includegraphics[scale=0.2]{it50-k3-l80.png}}
\end{tabular}
\caption{Pour k=3, $\lambda$=0.80}
\end{table}
\newpage


\begin{table}[h]
\begin{tabular}{cc}
\subfloat[Pour k=12 ; $\lambda=0.02$]{\includegraphics[scale=0.4]{MAE-least-squares-k12-l2.png}}
&
\subfloat[Pour k=12 ; $\lambda=0.80$]{\includegraphics[scale=0.4]{MAE-least-squares-k12-l80.png}}
\end{tabular}
\end{table}

\begin{table}[h]
\begin{tabular}{cccc}
\subfloat[X*Y à la $1^{ere}$ itération]{\includegraphics[scale=0.2]{it1-k12-l2.png}}
&
\subfloat[X*Y à la $2^e$ itération \textit{(meilleure MAE)}]{\includegraphics[scale=0.2]{it2-k12-l2.png}}
&
\subfloat[X*Y à la $3^e$ itération]{\includegraphics[scale=0.2]{it3-k12-l2.png}}
&
\subfloat[X*Y à la $10^e$ itération]{\includegraphics[scale=0.2]{it10-k12-l2.png}}
\end{tabular}
\caption{Pour k=12, $\lambda$=0.02}
\end{table}

\begin{table}[h]
\begin{tabular}{cccc}
\subfloat[X*Y à la $1^{ere}$ itération]{\includegraphics[scale=0.2]{it1-k12-l80.png}}
&
\subfloat[X*Y à la $2^e$ itération]{\includegraphics[scale=0.2]{it2-k12-l80.png}}
&
\subfloat[X*Y à la $10^e$ itération]{\includegraphics[scale=0.2]{it10-k12-l80.png}}
&
\subfloat[X*Y à la $35^e$ itération]{\includegraphics[scale=0.2]{it35-k12-l80.png}}
\end{tabular}
\caption{Pour k=12, $\lambda$=0.80}
\end{table}

On voit qu'on observe une homogénéisation des notes au bout d'un certain nombre d'itérations.\\
Attention cependant: Il s'agit possiblement de fausses observations dues à la façon dont les couleurs sont gérées ci-dessus. 
Si certains points très discrets sont loin hors de l'intervalle [0;5], ils pourraient faire passer toutes les notes de cet intervalle pour des notes "faibles" vertes.\\

\newpage

Pour ce qui est de la convergence de cet algorithme vers une solution, on obtient bien des solutions qui se stabilisent avec les itérations, et ceci quelques soient k et $\lambda$:

\begin{table}[h]
\begin{tabular}{cc}
\subfloat[à 5 itérations]{\includegraphics[scale=0.4]{MAE-least-squares-5.png}}
&
\subfloat[à 8 itérations]{\includegraphics[scale=0.4]{MAE-least-squares-8.png}}\\
\subfloat[à 10 itérations]{\includegraphics[scale=0.4]{MAE-least-squares-10.png}}
&
\subfloat[à 35 itérations]{\includegraphics[scale=0.4]{MAE-least-squares-35.png}}\\
\end{tabular}
\end{table}

\textit{Ces résultats ont été obtenus après de longues heures de calcul, grâce au module python "multiprocessing", voir le fichier "MAE-data.txt" fourni.}\\

On retiendra que les meilleurs résultats sont obtenus pour de petites valeurs de k et de grandes valeurs de $\lambda$.\\

On retiendra également qu'on converge bien vers un résultat stable lorsqu'on multiplie les itérations, cependant, on ne converge pas toujours vers une solution minimisant le plus la MAE.\\
C'est cohérent: $\lambda$ étant le paramètre 'distance' que l'on s'autorise à tolérer entre notre approximation $X\times Y$ et la matrice R, si l'on prend une grande valeur, on a besoin de moins d'itérations avant de trouver une solution qui fonctionne.\\
\newpage

\subparagraph{A propos du temps de calcul:}
Pour une itération, c'est à dire le calcul successif de X puis de Y, on procède à la résolution d'un système linéaire pour chaque ligne de X puis pour chaque colonne de Y:\\

Pour rappel, on a:\\
$$R = X.Y$$
$$(n\times m) = (n\times k)(k\times m)$$
\begin{figure}[h!]
\centering
\includegraphics[scale=0.5]{CodeCogsEqn.png}
\end{figure}

Ainsi, on résout à chaque itération (n+m) systèmes Ax=b de taille k:
$$
n\times \left[  \begin{pmatrix}
 &\dots  & \\ 
\vdots &  &\vdots \\ 
 &\dots  & 
\end{pmatrix}
\begin{pmatrix}
\\ 
\vdots \\ 
\\
\end{pmatrix}
=
\begin{pmatrix}
\\ 
\vdots \\ 
\\
\end{pmatrix}\right ]
+m\times \left[  \begin{pmatrix}
 &\dots  & \\ 
\vdots &  &\vdots \\ 
 &\dots  & 
\end{pmatrix}
\begin{pmatrix}
\\ 
\vdots \\ 
\\
\end{pmatrix}
=
\begin{pmatrix}
\\ 
\vdots \\ 
\\
\end{pmatrix}\right ]
$$

Là aussi, c'est cohérent avec les résultats observés sur la MAE: Plus la complexité du problème est grande (k est grand), plus l'approximation devient floue.
\newpage
\section{Question G - Comparaison des méthodes de prédiction}
On considère les méthodes avec les paramètres qui minimisent la MAE, à savoir k=12 pour l'approximation par SVD, et k=2, $\lambda$=0.85 et 20 itérations pour l'approximation par moindres carrés alternés:
\begin{table}[h]
\begin{tabular}{|l|c|c|}
\hline
 & Approximation par SVD & Approximation par moindres carrés\\
\hline
Temps de calcul de X et Y & $\approx$1 sec & $\approx$11 sec/iter (20 iter = 3 min) \\
\hline
Temps de prédiction & 40 $\mu sec$ & 9 $\mu sec$\\
\hline
MAE moyenne & 0.79 & 0.73 \\
\hline
\end{tabular}
\end{table}
\subparagraph{A propos du temps de prédiction}, les prédictions par SVD sont un peu plus longues à être calculées, puisqu'on a un double-produit de vecteurs au lieu d'un simple. Ce problème peut rapidement être résolu en calculant les matrices de features $X_k$ et $Y_k$, on aurait alors les mêmes temps de calcul pour chaque prédiction.\\

\subparagraph{Pour ce qui est du temps de construction}, la méthode des moindres carrés est beaucoup plus (trop) longue, pour n'obtenir au final une MAE que légèrement meilleure à la SVD.\\

Le principal intérêt de cette méthode est, comme on l'a vu sur les quelques images des produits X*Y précédentes, de pouvoir ajuster les paramètres pour augmenter ou diminuer l'homogénéité des notes. On peut, si on le souhaite, prendre k et $\lambda$ avec un nombre d'itérations précis de façon à obtenir très peu de notes fortement démarquées d'une masse moyenne. En bref, la seconde méthode est plus personnalisable.

\section{Question H - Amélioration des méthodes}
La première piste d'amélioration évidente serait de mélanger les deux méthodes: 
\begin{itemize}
\item Réaliser une approximation par SVD
\item Calculer les matrices de features associées ($X_k$ et $Y_k$)
\item Réutiliser ces matrices comme entrée pour les moindres carrés alternés
\end{itemize}
Ainsi, avec ne serait-ce qu'une seule itération des moindres carrés, on peut calculer des matrices de features avec une MAE de 0.74, et ceci en une trentaine de secondes.\\
Il s'agit d'un plutôt bon compromis.\\

\vspace{1cm}

Une deuxième piste d'amélioration pour la méthode des moindres carrés: On peut chercher à limiter la divergence des valeurs hors de l'intervalle [0,5] au fur et à mesure des itérations.\\
On pourrait alors:
\begin{itemize}
\item Soit remettre systématiquement à 5 les valeurs hors intervalle.
\item Soit normaliser ($ \times \frac{5}{max(R_{u,i})}$) complètement ces valeurs, de façon à toutes les ramener dans l'intervalle, en conservant la proportion de l'écart entre notes. Cette méthode demandera donc nécessairement encore plus de temps de calcul.
\end{itemize}
Il faudrait trouver un moyen de prédire qu'une valeur $R_{u,i}$ sera hors intervalle sans avoir à calculer explicitement la matrice R, ni avoir besoin de calculer toutes les prédictions une à une.\\
 
\section{Question I - Tester les Recommandations}
 
Pour ceci, rajoutons un utilisateur (nous) aux données, et inscrivons quelques notes personnalisées:\\

\begin{itemize}
\item Toy Story (1995) : 4
\item GoldenEye (1995) : 3
\item Braveheart (1995) : 5
\item Taxi Driver (1976) : 5
\item Batman Forever (1995) : 5
\item Star Wars (1977) : 5
\item Professional, The (1994) : 5
\item Stargate (1994) : 5
\item Maverick (1994) : 2
\item Fugitive, The (1993) : 4
\item Jurassic Park (1993) : 2
\item Blade Runner (1982) : 1
\item Aladdin (1992) : 5
\item Snow White and the Seven Dwarfs (1937) : 3
\item Top Gun (1986) : 5
\item Monty Python and the Holy Grail (1974) : 1
\item Return of the Jedi (1983) : 5
\item Dead Poets Society (1989) : 4
\item Cyrano de Bergerac (1990) : 5
\item Star Trek: Generations (1994) : 5
\item Mrs. Doubtfire (1993) : 2
\item Piano, The (1993) : 2
\item Hunchback of Notre Dame, The (1996) : 3
\item Microcosmos: Le peuple de l'herbe (1996) : 5
\item In the Line of Fire (1993) : 4
\item Pretty Woman (1990) : 4
\item Twilight (1998) : 1
\item Big Blue, The (Grand bleu, Le) (1988) : 5
\end{itemize}
\vspace{1cm}
Précisons ensuite notre numéro d'utilisateur et le nombre de recommandations que l'on souhaite, on peut donc récupérer les numéros des films ayant la meilleure prédiction estimée (que nous n'avons pas déjà noté):\\
Ici, pour 20 films, une approximation par SVD (k=12) et la matrice R remplie avec les moyennes de l'utilisateur:\\

\begin{itemize}
\item \begin{verbatim}Titanic (1997)  ==>[ item 313 ]\end{verbatim}
\item \begin{verbatim}Raiders of the Lost Ark (1981)  ==>[ item 174 ]\end{verbatim}
\item \begin{verbatim}Godfather, The (1972)  ==>[ item 127 ]\end{verbatim}
\item \begin{verbatim}Empire Strikes Back, The (1980)  ==>[ item 172 ]\end{verbatim}
\item \begin{verbatim}Forrest Gump (1994)  ==>[ item 69 ]\end{verbatim}
\item \begin{verbatim}L.A. Confidential (1997)  ==>[ item 302 ]\end{verbatim}
\item \begin{verbatim}Pulp Fiction (1994)  ==>[ item 56 ]\end{verbatim}
\item \begin{verbatim}Get Shorty (1995)  ==>[ item 4 ]\end{verbatim}
\item \begin{verbatim}Air Force One (1997)  ==>[ item 300 ]\end{verbatim}
\item \begin{verbatim}Firm, The (1993)  ==>[ item 77 ]\end{verbatim}
\item \begin{verbatim}Jerry Maguire (1996)  ==>[ item 237 ]\end{verbatim}
\item \begin{verbatim}Terminator 2: Judgment Day (1991)  ==>[ item 96 ]\end{verbatim}
\item \begin{verbatim}Net, The (1995)  ==>[ item 38 ]\end{verbatim}
\item \begin{verbatim}Indiana Jones and the Last Crusade (1989)  ==>[ item 210 ]\end{verbatim}
\item \begin{verbatim}English Patient, The (1996)  ==>[ item 286 ]\end{verbatim}
\item \begin{verbatim}Very Brady Sequel, A (1996)  ==>[ item 412 ]\end{verbatim}
\item \begin{verbatim}Beavis and Butt-head Do America (1996)  ==>[ item 240 ]\end{verbatim}
\item \begin{verbatim}Back to the Future (1985)  ==>[ item 204 ]\end{verbatim}
\item \begin{verbatim}Pocahontas (1995)  ==>[ item 542 ]\end{verbatim}
\item \begin{verbatim}Bram Stoker's Dracula (1992)  ==>[ item 217 ]\end{verbatim}
\end{itemize}
\vspace{1cm}
\textit{(Les recommandations sont calculées en 8 msec environ.)}\\
\begin{itemize}
\item Le résultat obtenu avec les moyennes des films est différent, et moins adapté malheureusement.
\item Avec l'approximation par moindres carrés alternés, la liste est plutôt bonne à 1 itération, mais moins bonne à 20 itérations.
\end{itemize}

\subparagraph{Conclusion:} Le système de recommandation fonctionne. Cependant, la MAE n'est pas un estimateur parfait de la qualité des prédictions, comme on le voit si dessus:\\

\begin{itemize}
\item La méthode de remplissage avec les moyennes des films donnait une meilleure MAE, mais des prédictions qui me plaisent moins
\item La méthode des moindres carrés, qui garde une MAE très stable au bout de plusieurs itérations, donne des listes de films de qualité très variable.
\end{itemize}

\textit{On peut donc conclure qu'il reste une grande part de hasard dans la prédiction.}
\end{document}
